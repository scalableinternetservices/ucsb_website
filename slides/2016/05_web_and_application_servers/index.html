<html>
  <head>
    <meta charset="utf-8" />
    <title>HTTP and Application Servers</title>
    <style>
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif);
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: 400;
        margin-bottom: 0;
      }
      .remark-slide-content h1 { font-size: 3em; }
      .remark-slide-content h2 { font-size: 2em; }
      .remark-slide-content h3 { font-size: 1.6em; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      li p { line-height: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      a, a > code {
        color: rgb(249, 38, 114);
        text-decoration: none;
      }
      blockquote {
        border-left: 0.3em solid rgba(0,0,0,0.5);
        padding: 0 15px;
        font-style: italic;
      }
      code {
        background: #e7e8e2;
        border-radius: 5px;
      }
      img {
        max-height: 100%;
        max-width: 100%;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-code-line-highlighted     { background-color: #373832; }
      .clear { clear: both; }
      #slideshow .slide .content code { font-size: 0.8em; }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .inverse {
        background: #272822;
        color: #777872;
        text-shadow: 0 0 20px #333;
      }
      .inverse h1, .inverse h2 {
        color: #f3f3f3;
        line-height: 0.8em;
      }
      .background-blue {
        background-color: deepskyblue;
      }
      .background-green {
        background-color: springgreen;
      }
      .background-pink {
        background-color: deeppink;
      }

      /* Two-column layout */
      .left-column {
        width: 49%;
        float: left;
      }
      .right-column {
        width: 49%;
        float: right;
      }
      .left-column20 {
        width: 20%;
        float: left;
      }
      .right-column80 {
        width: 79%;
        float: right;
      }
      .left-column30 {
        width: 30%;
        float: left;
      }
      .right-column70 {
        width: 69%;
        float: right;
      }
      .left-column40 {
        width: 40%;
        float: left;
      }
      .right-column60 {
        width: 59%;
        float: right;
      }

      /* Table layout */
      table {
        background: #f5f5f5;
        margin: 30px auto;
        text-align: left;
        width: 100%;
      }
      th {
        background: linear-gradient(#777, #444);
        color: #fff;
        font-weight: bold;
        padding: 10px 15px;
      }
      td {
        border-right: 1px solid #fff;
        border-left: 1px solid #e8e8e8;
        border-top: 1px solid #fff;
        border-bottom: 1px solid #e8e8e8;
        padding: 10px 15px;
      }

    </style>
  </head>
  <body>
    <textarea id="source">
class: center, middle

# HTTP and Application Servers

__CS291A__

Dr. Bryce Boe

October 6, 2015

---

# Today's Agenda

* TODO
* HTTP Server Architectures
* C10K Problem
* Application Servers

---

# TODO

## Should be done:

* Chapters 1, 2, 9-11 in
  [HPBN](https://hpbn.co/primer-on-latency-and-bandwidth/) / Chapters 1-6 in
  the [Ruby on Rails Tutorial](https://www.railstutorial.org/book/beginning)
* Familiarity with [git](http://rogerdudler.github.io/git-guide/)
* Have at least 20 user stories (40 would be better) for your project

## Before Tuesday's Class:

* Read
  [Dynamic Load Balancing on Web-server Systems](http://www.ics.uci.edu/~cs230/reading/DLB.pdf)
  by Cardellini, Colajanni, and Yu.

## Before Next Lab

* Complete at least #TEAM_MEMBERS stories that deliver value to your users
  (me).
* Complete chapters 7 through 10 in the
  [Ruby on Rails Tutorial](https://www.railstutorial.org/book/beginning)
* Hook TravisCI up to your repository.

---

# HTTP Server Architectures

* Single Threaded (no concurrency)
* Process per request
* Process pool
* Thread per request
* Process/thread worker pool
* Event-driven

For select examples see:
[https://gist.github.com/bboe/6a7b03fcd110c4c6bbe5ec412f523428](https://gist.github.com/bboe/6a7b03fcd110c4c6bbe5ec412f523428)

---

# Single Threaded HTTP Servers

```python
bind() to port 80 and listen()
loop forever
    accept() a socket connection
    while we can read from the socket
        read() a request
        process that request
        write() its response
    close() the socket connection
```

> What happens when another request comes in while we're within the loop?

---

# Single Threaded Problem

If a single threaded web service does not process the request quickly, other
clients end up waiting or dropping their connections.

We are building web applications not web sites. As a result:

* The requests are usually more complicated than serving a file from disk.
* It is common to have a web request doing a significant amount of computation
  and business logic.
* It is common to have a web request result in connections to multiple external
  services, e.g., databases, and caching stores.
* These requests can be anything: lightweight or heavyweight, IO intensive or
  CPU intensive.

We can solve these problems if the thread of control that processes the request
is separate from the thread that `listen()`s and `accept()`s new connections.

---

# Process per Request HTTP Server

Handle each request as a subprocess:

![forking web server](server_forking.png)

```python
bind() to port 80 and listen()
loop forever
    accept() a socket connection
*   if fork() == 0  # child process
        while we can read from the socket
            read() a request
            process that request
            write() its response
        close() the socket connection
*       exit()
```

---

# Process per Request HTTP Server

## Strengths

* Simple
* Provides easy isolation between requests
* No threading issues (to discuss)

## Weaknesses

* Does each request duplicate the process memory?
* What happens as the CPU load increases?
* How efficient is it to fire up a process on each request?
    * How much setup and tear down work is necessary?

---

# Process Pool HTTP Server

![process pool web server](server_process_pool.png)

Instead of spawning a process for each request create a pool of N processes at
start-up and have them handle incoming requests when available.

The children processes `accept()` the incoming connections and use shared
memory to coordinate.

The parent process watches the load on its children and can adjust the pool
size as needed.

---

# Process Pool HTTP Server

## Strengths

* Provides easy isolation between requests
* Children can die after _M_ requests to avoid memory leakage
* Process setup and tear down costs are minimized
* More predictable behavior under high load
* No threading issues (to discuss)

## Weaknesses

* More complex than process per request
* Many processes can still mean a large amount of memory consumption

This web server architecture is provided by the Apache 2.x MPM "Prefork" module.

---

# Thread per Request HTTP Server

Why use multiple processes at all? Instead we can use a single process and
spawn new threads for each request.

![http server thread per request](server_threaded.png)

```python
bind() to port 80 and listen()
loop forever
    accept() a socket connection
*   pthread_create()
        while we can read from the socket
            read() a request
            process that request
            write() its response
        close() the socket connection
*       # thread dies
```

---

# Thread per Request HTTP Server

## Strengths

* Relatively simple
* Reduced memory footprint compared to multi-processed

## Weaknesses

* Request handling code must be thread-safe
* Pushing thread-safety to the application developer is not ideal
* Setup and tear down needs to occur for each thread (or shared data
  needs to be thread-safe)
* Memory leaks?

---

# Process/Thread Worker Pool Server

![http process/thread worker pool](server_worker_pool.png)

Combination of the two techniques.

Master process spawns processes, each with many threads. Master maintains
process pool.

Processes coordinate through shared memory to `accept()` requests.

Fixed threads per request, scaling is done at the process level.

---

# Process/Thread Worker Pool Server

## Strengths

* Faults isolated between processes, but not threads
* Threads reduce memory footprint
* Tunable level of isolation
* Controlling the number of processes and threads allows for predictable
  behavior under load

## Weaknesses

* Requires thread-safe code
* Uses more memory than an all-thread based approach

This web server architecture is provided by the Apache 2.x MPM "Worker" module.

---

# C10K Problem

Originally posed in 1999 by Dan Kegel.

> Given a 1 GHz machine with 2GB of RAM, and a gigabit Ethernet card, can we
> support 10,000 simultaneous connections?

## 20,000 clients means each gets:

* 50 KHz of CPU
* 100 KB of RAM
* 50 Kb/second of network

"It shouldn't take any more horsepower than that to take four kilobytes from
the disk and send them to the network once a second for each of twenty thousand
clients."

> What makes managing concurrent connections difficult?

Source: [http://www.kegel.com/c10k.html](http://www.kegel.com/c10k.html)

---

# Each Client is...

* Reading from the network socket
* Parsing its request
* Opening a file on disk
* Reading the file into memory
* Writing the memory to network

---

# Each Client is... __blocking__

* Reading from the network socket (__blocking__)
* Parsing its request
* Opening a file on disk  (__blocking__)
* Read the file into memory  (__blocking__)
* Write the memory to network  (__blocking__)

---

# Waiting on I/O

Every time a process/thread is waiting on I/O it is not runnable, and it is not
cost-free:

* Each process/thread is considered every time the scheduler makes a decision
* Memory is occupied by the process, and its last load may have evicted other
  processes' memory from the cache

Massive concurrency slows down all processes/threads.

---

# How can we not wait on I/O?

Blocking system calls cause this problem.

> Can we accomplish our desired tasks without blocking?

## Yes! Asynchronous I/O

* `select()`: Provided a list of file descriptors, block only until at least
  one is ready for I/O (only usable up to 1024 file descriptors on Linux).
* `epoll_*()`: Register to listen for events on file descriptors. Again block
  only until at least one of the registered descriptors is ready for I/O

---

# Select example

Assume we have a list of sockets called fd_list.

```python
loop forever:
    select(fd_list, ...) // block until something has I/O to handle
    for fd in fd_list
        if fd is ready for IO
            handle_io(fd)
        else do nothing
```

## handle_io

* can include socket acceptance
* shouldn't make any blocking calls
    * Use their non-blocking variants
    * Rely on select/epoll to tell you when I/O is ready
* should avoid excessive computation
    * Use a separate thread, process, or worker pool for such purposes

---

# Event Driven Systems

Systems that operate in such a manner are called event driven system.

Often such systems can accomplish everything using only a single process and
thread, of course more may be needed for CPU-bound segments.

Well used examples:

* NGINX
* Tengine (fork of NGINX)
* lighttpd
* netty (java)
* node.js (JavaScript)
* eventmachine (ruby)
* twisted (python)

---

# Event Driven HTTP Servers

## Strengths

* High performance under high load
* Predictable performance under high load
* No need to be thread-proof (unless specifically adding thread-concurrency)

# Weaknesses

* Poor isolation
    * What happens if a bug causes an infinite loop?
* Fewer extensions, since code cannot use blocking syscalls
* Very complex

---

# Event Machine (Ruby) Example

Event driven code is dominated by callbacks:

```ruby
EM.run {
  page = EM::HttpRequest.new('http://google.ca/').get
  page.errback { p "Google is down! terminate?" }
  page.callback {
    a = EM::HttpRequest.new('http://google.ca/search?q=em').get
    a.callback { # callback nesting, ad infinitum }
    a.errback  { # error-handling code }
  }
}
```

---

# Callback Hell

![Yo dawg, I heard you like JavaScript](callback_hell.png)

It _can_ very easily become complicated.

---

# HTTP Server Architectures Review

* Single Threaded
* Process per request
    * Greatest isolation
    * Largest memory footprint
* Thread per request
    * Less isolation
    * Smaller memory footprint
* Process/thread worker pool
    * Tunable compromise between processes and threads
* Event-driven
    * Great performance under high load
    * Difficult to extend
    * Reduced isolation

---

# Application Servers

We are building web applications, so we will require complex server-side logic.

We _can_ extend our HTTP servers to provide this logic through modules, but
there are benefits to separating application servers to distinct process(es).

* Application logic will be dynamic
* Application logic regularly uses high level (slow) languages
* Security concerns are easier (HTTP server can shield app server from
  malformed requests)
* Setup costs can be amortized if the app server is running continuously

Instead we can have our HTTP server forward requests to the application
server(s).

---

# Inter-server Communication

> How does an HTTP Server communicate with the application server(s)?

A few different ways:

## CGI

Spawn a process, pass HTTP headers as ENV variables and utilize STDOUT as the
response.

## FastCGI, SCGI

Modifications to CGI to allow for persistent application server processes
(amortizes setup time).

## HTTP

Communicate via the HTTP protocol to a long-running process. (Essentially a
reverse-proxy configuration).

> Does it make sense to do this?

---

# Application Server Architectures

> What architecture should we use for our application server?

We have the same trade-offs to consider as with HTTP servers
(e.g. threads/processes/workers).

## Up Next

Let's take a quantitative look at various approaches with Ruby application
servers.

We will not consider evented ruby application servers (e.g., EventMachine)
because Rails will not run on such application servers.

---

# Our Test Setup

![Demo App](demo_app.png)

The Demo App is a link sharing website with:

* Multiple communities
* Each community can have many submissions
* Each submission can have a tree of comments

---

# Simulated Users

Using Tsung (erlang-based test framework) we will simulate multiple users
visiting the Demo App web service. Each user will:

```
Visit the homepage (/)
  Wait randomly between 0 and 2 seconds
Request community creation form
  Wait randomly between 0 and 2 seconds
Submit new community form
Request new link submission form
  Wait randomly between 0 and 2 seconds
Submit new link submission form
  Wait randomly between 0 and 2 seconds
Delete the link
  Wait randomly between 0 and 2 seconds
Delete the community
```

---

# Test Process

There are six phases of testing each lasting 60 seconds:

1. (0-59s) Every second a new simulated user arrives
2. (60-119s) Every second 1.5 new simulated users arrive
3. (120-179s) Every second 2 new simulated users arrive
4. (180-239s) Every second 2.5 new simulated users arrive
5. (240-299s) Every second 3 new simulated users arrive
6. (300-359s) Every second 3.5 new simulated users arrive

__Note__: Each user corresponds to seven requests and a user may wait up to ten
seconds with the delays.

---

# Test Environment

All tests were conducted on a single Amazon EC2 m3-medium instance.

* 1 vCPU
* 3.75 GB RAM

The tests used the `Puma` web application server (unless otherwise specified).

The `database_optimizations` branch of the demo app was used to run the tests:
[https://github.com/scalableinternetservices/demo/tree/database_optimizations](https://github.com/scalableinternetservices/demo/tree/database_optimizations)

---

# Single Thread/Process (Users)

![Single Thread/Process Users](demo_single_users.png)

---

# Single Thread/Process (Page Load)

Decrease in performance around 60s (1.5 new users per second)

![Single Thread/Process Page Load](demo_single_page_load.png)

---

# Four Processes (Users)

![Four Processes Users](demo_four_users.png)

---

# Four Processes (Page Load)

Decrease in performance around 240s (3 new users per second)

![Four Processes Page Load](demo_four_load.png)

---

# Sixteen Processes (Users)

![Sixteen Processes Users](demo_sixteen_users.png)

---

# Sixteen Processes (Page Load)

Little improvement over 4 processes

![Sixteen Processes Page Load](demo_sixteen_load.png)

---

# Threads instead of processes?

> What do you think will happen?

---

# Four Threads (Users)

![Four Threads Users](demo_four_threads_users.png)

---

# Four Threads (Page Load)

Still decrease in performance around 240s, but more
stable until then.

![Four Threads Page Load](demo_four_threads_load.png)

---

# 32 Threads (Users)

![32 Threads Users](demo_32_threads_users.png)

---

# 32 Threads (Page Load)

Decrease in performance beginning around 300s (3.5 new users per second)

![32 Threads Page Load](demo_32_threads_load.png)

---

# Side note: Ruby interpreters

There are different versions of the Ruby interpreter. Different workloads may
benefit from using different interpreters.

## MRI (Matz's Ruby Interpreter)

* The reference version
* Written in C
* Has a global interpreter lock (GIL) that prevents true thread-concurrency

## JRuby

* Written in Java
* Does not have GIL

---

# Application Server Options

In this class you will be able to compare the performance of a handful of
application servers:

## via elastic beanstalk configuration (formerly cloud formation templates)

* Puma (worker pool)
* Phusion Passenger (worker pool)

---

# Puma

Originally designed for Rubinius (GIL-less ruby interpreter).

Claims to require less memory than others (for the server itself)

Specifically made to work with thread-based parallelism, but also supports
multiple processes each with a tunable number of threads.

---

# Phusion Passenger

Passenger is a ruby web application server that can be added as a module to
either Apache or NGINX.

Passenger works as a worker pool adjusting the number of processes that
handle requests. Originally did not support threads within the processes.

---

# Puma and Passenger

Both Puma and Phusion Passenger:

* are relatively easy to configure
* enable processes to be forked after ruby/rails is loaded

> Why might you want to wait to load the application prior to forking?

---

# Thread Safety Note

If you can use thread-parallelism, do it! But, making your code thread safe
isn't always obvious.

## Old example: pp/models/order.rb

```ruby
class Order
  belongs_to :user, klass: User
end
```

The above code may result in _autoloading_ the `user.rb` for the `User`
model. In versions of ruby prior to 2.0 (February 2013) autoloading was not
thread safe.

> What else might not be thread safe?

Things to consider:

* Your code
* Your many dependencies

    </textarea>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"
            type="text/javascript"></script>
    <script type="text/javascript">
      var hljs = remark.highlighter.engine;
      var slideshow = remark.create({
          highlightLanguage: 'ruby',
          highlightLines: true,
          hightlightSpans: true,
          highlightStyle: 'monokai'
        }) ;
    </script>
  </body>
</html>
